# -*- coding: utf-8 -*-
"""
Created on Thu Jun 12 22:03:10 2014

@author: Weizhi
"""
import csv
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn import cross_validation 
def OpenCVS(path):
    f = open(path,'rb')
    reader = csv.reader(f)
    labels = []
    for row in reader:
        labels.append(row)
    f.close()
    data = np.array(labels)
    return data
    
GroupPath = 'C:\Users\Weizhi\Desktop\Volumes\Feature Selection\Train\\train_labels.csv'
train_SBM = 'C:\Users\Weizhi\Desktop\Volumes\Feature Selection\Train\\train_SBM.csv'
train_FNC = 'C:\Users\Weizhi\Desktop\Volumes\Feature Selection\Train\\train_FNC.csv'

group = OpenCVS(GroupPath)
SBM = OpenCVS(train_SBM)
FNC = OpenCVS(train_FNC)


## training the model
Group = group[1:,1:]
feature = np.column_stack((SBM[1:,1:],FNC[1:,1:]))

sh = feature.shape
feature= np.array([float(i) for i in feature.ravel() ]).reshape(sh)




rf = RandomForestClassifier(n_jobs=2)

Group_test = Group.astype(np.int)
result = rf.fit(feature,Group_test.ravel())

std = np.std([tree.feature_importances_ for tree in result.estimators_],
             axis=0)


importance = result.feature_importances_
indices = np.argsort(importance)[::-1]

import pylab as pl
from sklearn.metrics import roc_curve, auc


Number = 20

pl.figure()
pl.title("Feature Importance")
pl.bar(range(Number),importance[indices[:Number]])

pl.xticks(range(Number), indices)
pl.xlim([-1, Number])
pl.show()
indices[:Number]

## feature selections:
#from sklearn import cross_validation
#from sklearn.linear_model import SGDClassifier

def simple_crit_func(model,feat_sub,group):
    X_train, X_test, y_train, y_test = cross_validation.train_test_split(feat_sub,group,test_size=0.4,random_state=0)
    model.fit(X_train,y_train)
    Correct = model.score(X_test,y_test)
    return Correct
#

def seq_forw_select(clf,features, group,max_k, print_steps=True):

    Len = features.shape[1]
    feat_sub = []
    result = []
    if max_k >= Len:
        max_k = Len
#        result = np.arange(len)
        return np.arange(Len)
    else:
     
        index = np.arange(Len)
        np.random.shuffle(index)
        # initial values
        crit_func_max = simple_crit_func(clf,features[:,[index[1]]],group)
        # store the results
        result.append(index[1])
     
        for x in range(Len):
             
            feat_sub.append(index[x])
            # feedback, forward algorithms
            crit_func_eval = simple_crit_func(clf,features[:,feat_sub],group)
     
            if crit_func_eval >crit_func_max:
                crit_func_max = crit_func_eval
                result.append(index[x])
                if print_steps:
                    print ('Accuary rate: %f, add feature: %d',crit_func_eval,index[x])
             
            feat_sub.pop()
            k = len(result)
            if k == max_k or crit_func_max >=0.95 :
                break;
        
    return result



from sklearn.linear_model import SGDClassifier



X = feature

y = Group_test.ravel()

clf = SGDClassifier(alpha=0.01,n_iter=100)


res_forw = seq_forw_select(clf,features=X,group=y,max_k=X.shape[1],print_steps=True)






## cross validation

X_train, X_test, y_train, y_test = cross_validation.train_test_split(feature[:,indices[:Number]],Group_test.ravel(),test_size=0.4,random_state=0)

# def the model

def modeling(model,X_train,X_test,y_train,y_test):
    clf = model.fit(X_train,y_train)
    probas_=clf.predict_proba(X_test)
    error = clf.score(X_test,y_test)
    
    print "The CV accurate rate is: %f" % error
    group = y_test.astype(np.int)

    fpr, tpr, thresholds = roc_curve(group,probas_[:,1],pos_label=1)
    
    roc_auc = auc(fpr,tpr)
    print "Area under the ROC curve : %f" % roc_auc
    
    pl.clf()
    pl.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
    pl.plot([0, 1], [0, 1], 'k--')
    pl.xlim([0.0, 1.0])
    pl.ylim([0.0, 1.0])
    pl.xlabel('False Positive Rate')
    pl.ylabel('True Positive Rate')
    pl.title('Receiver operating characteristic example')
    pl.legend(loc="lower right")
    pl.show()
    return (clf,probas_)
    
    
from sklearn import svm
clf = svm.SVC(kernel='linear', probability=True,)
clf,probas = modeling(clf,X_train,X_test,y_train,y_test)

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier
from sklearn import tree, neighbors

knn = neighbors.KNeighborsClassifier(15,weights='distance')

clf_knn,probas = modeling(knn,X_train,X_test,y_train,y_test)

#dtree = tree.DecisionTreeClassifier()
#clf_knn,probas = modeling(dtree,X_train,X_test,y_train,y_test)

gnb = GaussianNB()


clf_gnb,probas = modeling(gnb,X_train,X_test,y_train,y_test)

SGD = SGDClassifier(loss="log", alpha=0.01, n_iter=200, fit_intercept=True)
clf_SGD,probas = modeling(SGD,X_train,X_test,y_train,y_test)





### random forest tree
#rf = RandomForestClassifier(n_jobs=2)
#clf,probas = modeling(rf,X_train,X_test,y_train,y_test)
#
#
#importance = clf.feature_importances_
#indices = np.argsort(importance)[::-1]
#
#import pylab as pl
#
#Number = 100
#
#pl.figure()
#pl.title("Feature Importance")
#pl.bar(range(Number),importance[indices[:Number]])
#
#pl.xticks(range(Number), indices)
#pl.xlim([-1, Number])
#pl.show()
#indices[:Number]
### cross validation

##





# reading the test data
test_SBM = 'C:\Users\Weizhi\Desktop\Volumes\Feature Selection\Test\\test_SBM.csv'
test_FNC = 'C:\Users\Weizhi\Desktop\Volumes\Feature Selection\Test\\test_FNC.csv'

test_SBM = OpenCVS(test_SBM)
test_FNC = OpenCVS(test_FNC)
test_feature = np.column_stack((test_SBM[1:,1:],test_FNC[1:,1:]))


probas_test = clf.predict_proba(test_feature[:,indices[:20]])

# get the outout
with open('C:\Users\Weizhi\Desktop\Volumes\Feature Selection\Test\\submission_example.csv', 'w') as fp:
    a = csv.writer(fp, delimiter=',')    
    data = np.column_stack((test_SBM[1:,:1],probas_test))
    title = ['ID','Probability']
    data1 = np.array(list(data).insert(0,title))
    
    a.writerows(data1)
f =  open('C:\Users\Weizhi\Desktop\Volumes\Feature Selection\Test\\submission_example.csv', 'wt')

try:
    writer = csv.writer(f)
    writer.writerow( ('Id', 'probability') )
    for row in len(data):
        writer.writerow(data[row,:])
finally:
    f.close()
    

    
